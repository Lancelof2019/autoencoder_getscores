{'state': 2, 'tid': 0, 'spec': None, 'result': {'loss': 0.864990234375, 'status': 'ok', 'params': {'alpha': 0.8439938436640144, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.0006185825636454771, 'learning_rate': 0.2655834967264489, 'optimizer': 'RMSProp', 'units1': 47, 'units2': 13}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.8305973>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=2.0764933>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=2.1624756>])}, 'misc': {'tid': 0, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [0], 'alpha2': [0], 'batch_size': [0], 'initializer': [0], 'lamda': [0], 'lamda2': [0], 'learning_rate': [0], 'optimizer': [0], 'units1': [0], 'units2': [0]}, 'vals': {'alpha': [1], 'alpha2': [0.8439938436640144], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.0006185825636454771], 'learning_rate': [0.2655834967264489], 'optimizer': [4], 'units1': [46], 'units2': [12]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 16, 42, 51, 203000), 'refresh_time': datetime.datetime(2024, 1, 2, 16, 48, 31, 525000)}
{'state': 2, 'tid': 1, 'spec': None, 'result': {'loss': 16.10987091064453, 'status': 'ok', 'params': {'alpha': 0.5971611067175878, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0.005325716580904299, 'learning_rate': 0.11661049789407663, 'optimizer': 'RMSProp', 'units1': 127, 'units2': 72}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=16.132181>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=40.330452>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=40.274677>])}, 'misc': {'tid': 1, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [1], 'alpha2': [1], 'batch_size': [1], 'initializer': [1], 'lamda': [1], 'lamda2': [1], 'learning_rate': [1], 'optimizer': [1], 'units1': [1], 'units2': [1]}, 'vals': {'alpha': [1], 'alpha2': [0.5971611067175878], 'batch_size': [2], 'initializer': [0], 'lamda': [1], 'lamda2': [0.005325716580904299], 'learning_rate': [0.11661049789407663], 'optimizer': [4], 'units1': [126], 'units2': [71]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 16, 48, 31, 542000), 'refresh_time': datetime.datetime(2024, 1, 2, 17, 3, 31, 585000)}
{'state': 2, 'tid': 2, 'spec': None, 'result': {'loss': 3336.403076171875, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.23096590788260662, 'learning_rate': 0.07572189519831701, 'optimizer': 'adam', 'units1': 298, 'units2': 186}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=3336.3274>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=8340.818>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=8341.008>])}, 'misc': {'tid': 2, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [2], 'alpha2': [], 'batch_size': [2], 'initializer': [2], 'lamda': [2], 'lamda2': [2], 'learning_rate': [2], 'optimizer': [2], 'units1': [2], 'units2': [2]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.23096590788260662], 'learning_rate': [0.07572189519831701], 'optimizer': [0], 'units1': [297], 'units2': [185]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 17, 3, 31, 599000), 'refresh_time': datetime.datetime(2024, 1, 2, 17, 10, 45, 538000)}
{'state': 2, 'tid': 3, 'spec': None, 'result': {'loss': 8.980923652648926, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0.0014802520566187139, 'learning_rate': 0.30697604619820884, 'optimizer': 'RMSProp', 'units1': 69, 'units2': 145}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=8.949358>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=22.373394>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=22.452309>])}, 'misc': {'tid': 3, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [3], 'alpha2': [], 'batch_size': [3], 'initializer': [3], 'lamda': [3], 'lamda2': [3], 'learning_rate': [3], 'optimizer': [3], 'units1': [3], 'units2': [3]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [2], 'initializer': [0], 'lamda': [1], 'lamda2': [0.0014802520566187139], 'learning_rate': [0.30697604619820884], 'optimizer': [4], 'units1': [68], 'units2': [144]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 17, 10, 45, 551000), 'refresh_time': datetime.datetime(2024, 1, 2, 17, 24, 34, 51000)}
{'state': 2, 'tid': 4, 'spec': None, 'result': {'loss': 0.5363311767578125, 'status': 'ok', 'params': {'alpha': 0.8061121028022185, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.03179166713269102, 'optimizer': 'adam', 'units1': 267, 'units2': 218}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.46578845>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.1644711>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.340828>])}, 'misc': {'tid': 4, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [4], 'alpha2': [4], 'batch_size': [4], 'initializer': [4], 'lamda': [4], 'lamda2': [], 'learning_rate': [4], 'optimizer': [4], 'units1': [4], 'units2': [4]}, 'vals': {'alpha': [1], 'alpha2': [0.8061121028022185], 'batch_size': [2], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.03179166713269102], 'optimizer': [0], 'units1': [266], 'units2': [217]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 17, 24, 34, 64000), 'refresh_time': datetime.datetime(2024, 1, 2, 17, 41, 55, 604000)}
{'state': 2, 'tid': 5, 'spec': None, 'result': {'loss': 0.49869808554649353, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.03882656377588483, 'optimizer': 'Momentum', 'units1': 43, 'units2': 169}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.48132592>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.2033148>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.2467452>])}, 'misc': {'tid': 5, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [5], 'alpha2': [], 'batch_size': [5], 'initializer': [5], 'lamda': [5], 'lamda2': [], 'learning_rate': [5], 'optimizer': [5], 'units1': [5], 'units2': [5]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [2], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.03882656377588483], 'optimizer': [3], 'units1': [42], 'units2': [168]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 17, 41, 55, 633000), 'refresh_time': datetime.datetime(2024, 1, 2, 17, 55, 38, 166000)}
{'state': 2, 'tid': 6, 'spec': None, 'result': {'loss': 2.9079596996307373, 'status': 'ok', 'params': {'alpha': 0.41936968693250454, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0.0009943850312148154, 'learning_rate': 0.32418093075131926, 'optimizer': 'nadam', 'units1': 15, 'units2': 118}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=2.8409002>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=7.1022506>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=7.2698994>])}, 'misc': {'tid': 6, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [6], 'alpha2': [6], 'batch_size': [6], 'initializer': [6], 'lamda': [6], 'lamda2': [6], 'learning_rate': [6], 'optimizer': [6], 'units1': [6], 'units2': [6]}, 'vals': {'alpha': [1], 'alpha2': [0.41936968693250454], 'batch_size': [2], 'initializer': [0], 'lamda': [1], 'lamda2': [0.0009943850312148154], 'learning_rate': [0.32418093075131926], 'optimizer': [1], 'units1': [14], 'units2': [117]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 17, 55, 38, 184000), 'refresh_time': datetime.datetime(2024, 1, 2, 18, 54, 34, 196000)}
{'state': 2, 'tid': 7, 'spec': None, 'result': {'loss': 393.3177795410156, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.04612775129190139, 'learning_rate': 0.024246410270416418, 'optimizer': 'adam', 'units1': 88, 'units2': 212}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=393.3238>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=983.30945>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=983.29443>])}, 'misc': {'tid': 7, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [7], 'alpha2': [], 'batch_size': [7], 'initializer': [7], 'lamda': [7], 'lamda2': [7], 'learning_rate': [7], 'optimizer': [7], 'units1': [7], 'units2': [7]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.04612775129190139], 'learning_rate': [0.024246410270416418], 'optimizer': [0], 'units1': [87], 'units2': [211]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 18, 54, 34, 221000), 'refresh_time': datetime.datetime(2024, 1, 2, 19, 9, 35, 720000)}
{'state': 2, 'tid': 8, 'spec': None, 'result': {'loss': 0.5194519758224487, 'status': 'ok', 'params': {'alpha': 0.8203847818315538, 'batch_size': 16, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.17218549729031946, 'optimizer': 'SGD', 'units1': 198, 'units2': 69}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.44823247>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.1205812>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.2986299>])}, 'misc': {'tid': 8, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [8], 'alpha2': [8], 'batch_size': [8], 'initializer': [8], 'lamda': [8], 'lamda2': [], 'learning_rate': [8], 'optimizer': [8], 'units1': [8], 'units2': [8]}, 'vals': {'alpha': [1], 'alpha2': [0.8203847818315538], 'batch_size': [0], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.17218549729031946], 'optimizer': [2], 'units1': [197], 'units2': [68]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 19, 9, 35, 747000), 'refresh_time': datetime.datetime(2024, 1, 2, 19, 15, 11, 887000)}
{'state': 2, 'tid': 9, 'spec': None, 'result': {'loss': 206.6688690185547, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0.014823123562633914, 'learning_rate': 0.07763597525963553, 'optimizer': 'SGD', 'units1': 189, 'units2': 277}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=206.66519>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=516.66296>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=516.6722>])}, 'misc': {'tid': 9, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [9], 'alpha2': [], 'batch_size': [9], 'initializer': [9], 'lamda': [9], 'lamda2': [9], 'learning_rate': [9], 'optimizer': [9], 'units1': [9], 'units2': [9]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [2], 'initializer': [0], 'lamda': [1], 'lamda2': [0.014823123562633914], 'learning_rate': [0.07763597525963553], 'optimizer': [2], 'units1': [188], 'units2': [276]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 19, 15, 12, 18000), 'refresh_time': datetime.datetime(2024, 1, 2, 19, 28, 54, 580000)}
{'state': 2, 'tid': 10, 'spec': None, 'result': {'loss': 0.5399686098098755, 'status': 'ok', 'params': {'alpha': 0.5110522683320942, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.09287764554251021, 'optimizer': 'adam', 'units1': 186, 'units2': 178}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.46231818>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.1557955>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.3499216>])}, 'misc': {'tid': 10, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [10], 'alpha2': [10], 'batch_size': [10], 'initializer': [10], 'lamda': [10], 'lamda2': [], 'learning_rate': [10], 'optimizer': [10], 'units1': [10], 'units2': [10]}, 'vals': {'alpha': [1], 'alpha2': [0.5110522683320942], 'batch_size': [2], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.09287764554251021], 'optimizer': [0], 'units1': [185], 'units2': [177]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 19, 28, 54, 598000), 'refresh_time': datetime.datetime(2024, 1, 2, 19, 58, 23, 809000)}
{'state': 2, 'tid': 11, 'spec': None, 'result': {'loss': 107.95941162109375, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.007252632528289822, 'learning_rate': 0.07134640725340291, 'optimizer': 'adam', 'units1': 252, 'units2': 242}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=107.978065>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=269.94516>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=269.89853>])}, 'misc': {'tid': 11, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [11], 'alpha2': [], 'batch_size': [11], 'initializer': [11], 'lamda': [11], 'lamda2': [11], 'learning_rate': [11], 'optimizer': [11], 'units1': [11], 'units2': [11]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.007252632528289822], 'learning_rate': [0.07134640725340291], 'optimizer': [0], 'units1': [251], 'units2': [241]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 19, 58, 23, 853000), 'refresh_time': datetime.datetime(2024, 1, 2, 20, 15, 30, 207000)}
{'state': 2, 'tid': 12, 'spec': None, 'result': {'loss': 0.4840245246887207, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.18860928445748248, 'optimizer': 'RMSProp', 'units1': 252, 'units2': 218}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.43804368>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.0951092>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.2100613>])}, 'misc': {'tid': 12, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [12], 'alpha2': [], 'batch_size': [12], 'initializer': [12], 'lamda': [12], 'lamda2': [], 'learning_rate': [12], 'optimizer': [12], 'units1': [12], 'units2': [12]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [1], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.18860928445748248], 'optimizer': [4], 'units1': [251], 'units2': [217]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 2, 20, 15, 30, 230000), 'refresh_time': datetime.datetime(2024, 1, 3, 6, 39, 43, 78000)}
{'state': 2, 'tid': 13, 'spec': None, 'result': {'loss': 122.68998718261719, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.01575101367450915, 'learning_rate': 0.008000755966362834, 'optimizer': 'SGD', 'units1': 178, 'units2': 98}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=122.72349>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=306.80872>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=306.72498>])}, 'misc': {'tid': 13, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [13], 'alpha2': [], 'batch_size': [13], 'initializer': [13], 'lamda': [13], 'lamda2': [13], 'learning_rate': [13], 'optimizer': [13], 'units1': [13], 'units2': [13]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.01575101367450915], 'learning_rate': [0.008000755966362834], 'optimizer': [2], 'units1': [177], 'units2': [97]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 6, 39, 43, 103000), 'refresh_time': datetime.datetime(2024, 1, 3, 6, 45, 21, 593000)}
{'state': 2, 'tid': 14, 'spec': None, 'result': {'loss': 0.4356074929237366, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 16, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.12291904952244118, 'optimizer': 'RMSProp', 'units1': 192, 'units2': 98}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.46915346>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.1728836>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.0890187>])}, 'misc': {'tid': 14, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [14], 'alpha2': [], 'batch_size': [14], 'initializer': [14], 'lamda': [14], 'lamda2': [], 'learning_rate': [14], 'optimizer': [14], 'units1': [14], 'units2': [14]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [0], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.12291904952244118], 'optimizer': [4], 'units1': [191], 'units2': [97]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 6, 45, 21, 613000), 'refresh_time': datetime.datetime(2024, 1, 3, 6, 49, 36, 573000)}
{'state': 2, 'tid': 15, 'spec': None, 'result': {'loss': 23.088138580322266, 'status': 'ok', 'params': {'alpha': 0.3553388529841678, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0.0019257849271747576, 'learning_rate': 0.12297700624744652, 'optimizer': 'Momentum', 'units1': 254, 'units2': 292}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=23.085583>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=57.713955>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=57.720345>])}, 'misc': {'tid': 15, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [15], 'alpha2': [15], 'batch_size': [15], 'initializer': [15], 'lamda': [15], 'lamda2': [15], 'learning_rate': [15], 'optimizer': [15], 'units1': [15], 'units2': [15]}, 'vals': {'alpha': [1], 'alpha2': [0.3553388529841678], 'batch_size': [2], 'initializer': [0], 'lamda': [1], 'lamda2': [0.0019257849271747576], 'learning_rate': [0.12297700624744652], 'optimizer': [3], 'units1': [253], 'units2': [291]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 6, 49, 36, 592000), 'refresh_time': datetime.datetime(2024, 1, 3, 7, 7, 59, 489000)}
{'state': 2, 'tid': 16, 'spec': None, 'result': {'loss': 1.9803968667984009, 'status': 'ok', 'params': {'alpha': 0.2080440142284442, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.0004004118378390107, 'learning_rate': 0.1384621575811711, 'optimizer': 'SGD', 'units1': 74, 'units2': 99}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=1.9680536>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=4.920134>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=4.950992>])}, 'misc': {'tid': 16, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [16], 'alpha2': [16], 'batch_size': [16], 'initializer': [16], 'lamda': [16], 'lamda2': [16], 'learning_rate': [16], 'optimizer': [16], 'units1': [16], 'units2': [16]}, 'vals': {'alpha': [1], 'alpha2': [0.2080440142284442], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.0004004118378390107], 'learning_rate': [0.1384621575811711], 'optimizer': [2], 'units1': [73], 'units2': [98]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 7, 7, 59, 505000), 'refresh_time': datetime.datetime(2024, 1, 3, 7, 13, 31, 765000)}
{'state': 2, 'tid': 17, 'spec': None, 'result': {'loss': 107.59150695800781, 'status': 'ok', 'params': {'alpha': 0.09705819120748649, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.016683434088141217, 'learning_rate': 0.019490053280823514, 'optimizer': 'RMSProp', 'units1': 88, 'units2': 163}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=107.66235>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=269.15588>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=268.97876>])}, 'misc': {'tid': 17, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [17], 'alpha2': [17], 'batch_size': [17], 'initializer': [17], 'lamda': [17], 'lamda2': [17], 'learning_rate': [17], 'optimizer': [17], 'units1': [17], 'units2': [17]}, 'vals': {'alpha': [1], 'alpha2': [0.09705819120748649], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.016683434088141217], 'learning_rate': [0.019490053280823514], 'optimizer': [4], 'units1': [87], 'units2': [162]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 7, 13, 31, 777000), 'refresh_time': datetime.datetime(2024, 1, 3, 7, 21, 32, 685000)}
{'state': 2, 'tid': 18, 'spec': None, 'result': {'loss': 550.071044921875, 'status': 'ok', 'params': {'alpha': 0, 'batch_size': 8, 'initializer': 'xavier', 'lamda': 0.26646029669824833, 'learning_rate': 0.2890321869407478, 'optimizer': 'nadam', 'units1': 46, 'units2': 52}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=549.95856>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1374.8964>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1375.1776>])}, 'misc': {'tid': 18, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [18], 'alpha2': [], 'batch_size': [18], 'initializer': [18], 'lamda': [18], 'lamda2': [18], 'learning_rate': [18], 'optimizer': [18], 'units1': [18], 'units2': [18]}, 'vals': {'alpha': [0], 'alpha2': [], 'batch_size': [1], 'initializer': [0], 'lamda': [1], 'lamda2': [0.26646029669824833], 'learning_rate': [0.2890321869407478], 'optimizer': [1], 'units1': [45], 'units2': [51]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 7, 21, 32, 699000), 'refresh_time': datetime.datetime(2024, 1, 3, 7, 29, 30, 182000)}
{'state': 2, 'tid': 19, 'spec': None, 'result': {'loss': 0.48390355706214905, 'status': 'ok', 'params': {'alpha': 0.11050197671479434, 'batch_size': 4, 'initializer': 'xavier', 'lamda': 0, 'learning_rate': 0.014259966992264157, 'optimizer': 'RMSProp', 'units1': 271, 'units2': 206}, 'loss_train': <tf.Tensor: shape=(), dtype=float32, numpy=0.4771604>, 'history_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.192901>]), 'history_val_loss': ListWrapper([<tf.Tensor: shape=(), dtype=float32, numpy=1.2097589>])}, 'misc': {'tid': 19, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'alpha': [19], 'alpha2': [19], 'batch_size': [19], 'initializer': [19], 'lamda': [19], 'lamda2': [], 'learning_rate': [19], 'optimizer': [19], 'units1': [19], 'units2': [19]}, 'vals': {'alpha': [1], 'alpha2': [0.11050197671479434], 'batch_size': [2], 'initializer': [0], 'lamda': [0], 'lamda2': [], 'learning_rate': [0.014259966992264157], 'optimizer': [4], 'units1': [270], 'units2': [205]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 1, 3, 7, 29, 30, 202000), 'refresh_time': datetime.datetime(2024, 1, 3, 7, 55, 59, 529000)}
